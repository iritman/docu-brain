#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ุชุณุช ุณุณุชู ุชููุฏ ุณูุงู ฺูุงุฑ ฺฏุฒููโุง
ุงู ูุงู ุจุฑุง ุขุฒูุงุด ุนููฺฉุฑุฏ ุณุณุชู ุฏุฑ ุชููุฏ ุณูุงูุงุช ฺูุงุฑ ฺฏุฒููโุง ุงุณุชูุงุฏู ูโุดูุฏ
"""

import os
from dotenv import load_dotenv
from modules.llm_client import LLMClient

# ุจุงุฑฺฏุฐุงุฑ ูุชุบุฑูุง ูุญุท
load_dotenv()

def test_mcq_generation():
    """ุชุณุช ุชููุฏ ุณูุงู ฺูุงุฑ ฺฏุฒููโุง"""
    
    # ุงุฌุงุฏ ฺฉูุงูุช LLM
    llm_client = LLMClient()
    
    # ูุชู ููููู ุจุฑุง ุชุณุช
    sample_context = """
    ููุด ูุตููุน (Artificial Intelligence) ุดุงุฎูโุง ุงุฒ ุนููู ฺฉุงููพูุชุฑ ุงุณุช ฺฉู ุจู ุงุฌุงุฏ ุณุณุชูโูุง ูโูพุฑุฏุงุฒุฏ ฺฉู ูุงุฏุฑ ุจู ุงูุฌุงู ฺฉุงุฑูุง ูุณุชูุฏ ฺฉู ูุนูููุงู ูุงุฒ ุจู ููุด ุงูุณุงู ุฏุงุฑูุฏ. ุงู ฺฉุงุฑูุง ุดุงูู ุงุฏฺฏุฑุ ุงุณุชุฏูุงูุ ุฏุฑฺฉ ุฒุจุงู ุทุจุนุ ุจูุง ฺฉุงููพูุชุฑ ู ุชุตููโฺฏุฑ ูโุดูุฏ.
    
    ุงููุงุน ุงุตู ููุด ูุตููุน ุนุจุงุฑุชูุฏ ุงุฒ:
    1. ููุด ูุตููุน ุถุนู (Narrow AI): ฺฉู ุจุฑุง ุงูุฌุงู ฺฉ ฺฉุงุฑ ุฎุงุต ุทุฑุงุญ ุดุฏู ุงุณุช
    2. ููุด ูุตููุน ูู (General AI): ฺฉู ูุงุฏุฑ ุจู ุงูุฌุงู ูุฑ ฺฉุงุฑ ุงุณุช ฺฉู ุงูุณุงู ูโุชูุงูุฏ ุงูุฌุงู ุฏูุฏ
    3. ููุด ูุตููุน ูููโุงูุนุงุฏู (Super AI): ฺฉู ุงุฒ ุชูุงูุงโูุง ุงูุณุงู ูุฑุงุชุฑ ูโุฑูุฏ
    
    ุงุฏฺฏุฑ ูุงุดู (Machine Learning) ุฒุฑูุฌููุนูโุง ุงุฒ ููุด ูุตููุน ุงุณุช ฺฉู ุจู ุณุณุชูโูุง ุงุฌุงุฒู ูโุฏูุฏ ุจุฏูู ุจุฑูุงููโููุณ ุตุฑุญุ ุงุฒ ุฏุงุฏูโูุง ุงุฏ ุจฺฏุฑูุฏ ู ุนููฺฉุฑุฏ ุฎูุฏ ุฑุง ุจูุจูุฏ ุฏููุฏ.
    """
    
    # ุชุณุชโูุง ูุฎุชูู
    test_questions = [
        "ุงุฒ ุงููุงุน ููุด ูุตููุนุ ุณูุงู ฺูุงุฑ ฺฏุฒููโุง ุจุณุงุฒ",
        "ุณูุงู ุชุณุช ุฏุฑ ููุฑุฏ ุชุนุฑู ููุด ูุตููุน ุทุฑุงุญ ฺฉู",
        "ฺูุงุฑ ฺฏุฒููโุง ุงุฒ ููููู ุงุฏฺฏุฑ ูุงุดู",
        "ุณูุงู ฺฉูฺฉูุฑ ุงุฒ ุงู ูุชู ุจุณุงุฒ",
        "ุชุณุช ฺูุงุฑฺฏุฒููโุง ุงุฒ ุงููุงุน AI"
    ]
    
    print("=" * 80)
    print("ุชุณุช ุณุณุชู ุชููุฏ ุณูุงู ฺูุงุฑ ฺฏุฒููโุง")
    print("=" * 80)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n๐ ุชุณุช {i}: {question}")
        print("-" * 60)
        
        try:
            # ุชููุฏ ูพุงุณุฎ
            response = llm_client.generate_response(sample_context, question)
            print(f"โ ูพุงุณุฎ ุชููุฏ ุดุฏ:")
            print(response)
            
        except Exception as e:
            print(f"โ ุฎุทุง ุฏุฑ ุชููุฏ ูพุงุณุฎ: {str(e)}")
        
        print("\n" + "=" * 80)

def test_normal_question():
    """ุชุณุช ุณูุงู ูุนููู ุจุฑุง ููุงุณู"""
    
    llm_client = LLMClient()
    
    sample_context = """
    ููุด ูุตููุน ุดุงุฎูโุง ุงุฒ ุนููู ฺฉุงููพูุชุฑ ุงุณุช ฺฉู ุจู ุงุฌุงุฏ ุณุณุชูโูุง ูโูพุฑุฏุงุฒุฏ ฺฉู ูุงุฏุฑ ุจู ุงูุฌุงู ฺฉุงุฑูุง ูุณุชูุฏ ฺฉู ูุนูููุงู ูุงุฒ ุจู ููุด ุงูุณุงู ุฏุงุฑูุฏ.
    """
    
    normal_question = "ููุด ูุตููุน ฺุณุชุ"
    
    print("\n๐ ุชุณุช ุณูุงู ูุนููู:")
    print("-" * 40)
    print(f"ุณูุงู: {normal_question}")
    
    try:
        response = llm_client.generate_response(sample_context, normal_question)
        print(f"โ ูพุงุณุฎ: {response}")
    except Exception as e:
        print(f"โ ุฎุทุง: {str(e)}")

if __name__ == "__main__":
    print("ุดุฑูุน ุชุณุช ุณุณุชู...")
    
    # ุจุฑุฑุณ ูุฌูุฏ ฺฉูุฏ API
    if not os.getenv("OPENROUTER_API_KEY"):
        print("โ ุฎุทุง: ฺฉูุฏ API ุฏุฑ ูุงู .env ุชูุธู ูุดุฏู ุงุณุช")
        exit(1)
    
    # ุชุณุช ุณูุงูุงุช ฺูุงุฑ ฺฏุฒููโุง
    test_mcq_generation()
    
    # ุชุณุช ุณูุงู ูุนููู
    test_normal_question()
    
    print("\nโ ุชุณุชโูุง ุชฺฉูู ุดุฏ!")